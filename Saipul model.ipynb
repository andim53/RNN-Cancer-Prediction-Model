{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516ec119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "#mostly read file function\n",
    "import numpy  \n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from numpy import sqrt\n",
    "\n",
    "from skimage.transform import pyramid_reduce, resize\n",
    "\n",
    "#img function\n",
    "import cv2 as cv\n",
    "#thresholding filtering\n",
    "import skimage.filters as filters\n",
    "\n",
    "#ploting img or diagram function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#train test split(split the dataset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#img preprocessing function\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#shuffle the list\n",
    "from sklearn.utils import shuffle\n",
    "                        \n",
    "#to get the optimal parameter for the model\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#getting result tools\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "\n",
    "#preprocessing part\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label,regionprops, perimeter\n",
    "from skimage.morphology import ball, disk, binary_erosion, remove_small_objects, reconstruction, binary_closing, binary_opening\n",
    "from skimage.filters import roberts, sobel\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "#plot 3d\n",
    "from skimage import measure, feature\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6f6aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  42\n",
      "30\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "#get all classes names\n",
    "classes = [\"Bengin\",\"Malignant\",\"Normal\"]\n",
    "\n",
    "#get all images location(total 1097 img)\n",
    "img_list = sorted(glob.glob('Dataset/*/*.*'))\n",
    "print('Total: ', len(img_list))\n",
    "\n",
    "#get all images location(total 1097 img)\n",
    "# train_list = sorted(glob.glob('Dataset/Train/*.*'))\n",
    "train_list = sorted(glob.glob('Dataset/Train/*.*'))\n",
    "print(len(train_list))\n",
    "\n",
    "#get all images location(total 1097 img)\n",
    "# test_list = sorted(glob.glob('Dataset/Test/*.*'))\n",
    "test_list = sorted(glob.glob('Dataset/Test/*.*'))\n",
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da75ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variable to hold X & y\n",
    "#create numpy array placeholder for pixels with 1 channel(grayscale)\n",
    "IMG_SIZE = 512\n",
    "CHANNEL = 1\n",
    "#arg: (length of numpy set, height, width, color channel)\n",
    "X = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "\n",
    "y = []\n",
    "\n",
    "# convert images to numpy arrays\n",
    "for i, img_path in enumerate(img_list):\n",
    "    # load image\n",
    "    img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "    img = cv.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    X[i] = img\n",
    "    y.append(\"Bengin\") if img_path.find(classes[0]) != -1 else (y.append(classes[1]) if img_path.find(classes[1]) != -1 else y.append(classes[2]))\n",
    "    \n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc38af",
   "metadata": {},
   "source": [
    "# Segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5835766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmented_lungs(im, num, save=False, plot=False, show_on_window=False, crop_percentage=0.05):\n",
    "    #This funtion segments the lungs from the given 2D slice.\n",
    "    \n",
    "    crop = im.copy()\n",
    "    if show_on_window:\n",
    "        height,width=im.shape[:2]\n",
    "        start_row,start_col=int(height*crop_percentage),int(width*crop_percentage)\n",
    "        end_row,end_col=int(height*(1-crop_percentage)),int(width*(1-crop_percentage))\n",
    "        crop=crop[start_row:end_row,start_col:end_col]\n",
    "    else:\n",
    "        if num == 161 or (num >= 173 and num <= 174) or (num == 758):\n",
    "            height,width=im.shape[:2]\n",
    "            start_row,start_col=int(height*0.20),int(width*0.20)\n",
    "            end_row,end_col=int(height*0.80),int(width*0.80)\n",
    "            crop=crop[start_row:end_row,start_col:end_col]\n",
    "        elif num >= 756 and num <= 767:\n",
    "            #Step 1: Crop the image \n",
    "            height,width=im.shape[:2]\n",
    "            start_row,start_col=int(height*0),int(width*0)\n",
    "            end_row,end_col=int(height*1),int(width*1)\n",
    "            crop=crop[start_row:end_row,start_col:end_col]\n",
    "        elif num == 1320 or num == 1219 or (num >= 712 and num <= 767) or (num >= 779 and num <= 799) or (num >= 688 and num <= 699) or (num >= 648 and num <= 664) or (num >= 225 and num <= 234):\n",
    "            #Step 1: Crop the image \n",
    "            height,width=im.shape[:2]\n",
    "            start_row,start_col=int(height*0.03),int(width*0.03)\n",
    "            end_row,end_col=int(height*0.97),int(width*0.97)\n",
    "            crop=crop[start_row:end_row,start_col:end_col]\n",
    "        else:\n",
    "            #Step 1: Crop the image \n",
    "            height,width=im.shape[:2]\n",
    "            start_row,start_col=int(height*0.12),int(width*0.12)\n",
    "            end_row,end_col=int(height*0.88),int(width*0.88)\n",
    "            crop=crop[start_row:end_row,start_col:end_col]\n",
    "        \n",
    "    #Step 2: Convert into a binary image. \n",
    "    ret,binary = cv.threshold(crop,140,255,cv.THRESH_BINARY_INV)\n",
    "    \n",
    "    #Step 3: Remove the blobs connected to the border of the image.\n",
    "    cleared = clear_border(binary) \n",
    "    \n",
    "    #Step 4: Closure operation with a disk of radius 10. This operation is \n",
    "    #to keep nodules attached to the lung wall.\n",
    "    selem = disk(2)\n",
    "    closing = binary_closing(cleared, selem)\n",
    "        \n",
    "    #Step 5: Label the image.\n",
    "    label_image = label(closing)\n",
    "    \n",
    "    #Step 6: Keep the labels with 2 largest areas.\n",
    "    areas = [r.area for r in regionprops(label_image)]\n",
    "    areas.sort()\n",
    "    if len(areas) > 2:\n",
    "        for region in regionprops(label_image):\n",
    "            if region.area < areas[-2]:\n",
    "                for coordinates in region.coords:                \n",
    "                       label_image[coordinates[0], coordinates[1]] = 0\n",
    "    segmented_area = label_image > 0\n",
    "        \n",
    "    #Step 7: Erosion operation with a disk of radius 2. This operation is \n",
    "    #seperate the lung nodules attached to the blood vessels.\n",
    "    selem = disk(2)\n",
    "    erosion = binary_erosion(segmented_area, selem) \n",
    "        \n",
    "    #     Step 4: Closure operation with a disk of radius 10. This operation is \n",
    "    #     to keep nodules attached to the lung wall.\n",
    "    selem = disk(10)\n",
    "    closing2 = binary_closing(erosion, selem)    \n",
    "    \n",
    "    #Step 8: Fill in the small holes inside the binary mask of lungs.\n",
    "    edges = roberts(closing2)\n",
    "    fill_holes = ndi.binary_fill_holes(edges)\n",
    "        \n",
    "    superimpose = crop.copy()\n",
    "    #Step 9: Superimpose叠加 the binary mask on the input image.\n",
    "    get_high_vals = fill_holes == 0\n",
    "    superimpose[get_high_vals] = 0\n",
    "\n",
    "    superimpose = cv.resize(superimpose, (512, 512))    \n",
    "    \n",
    "    if show_on_window:\n",
    "        directory1 = 'result/'\n",
    "        directory2 = '.jpg'\n",
    "        images = [im, crop, binary, cleared, closing, segmented_area, erosion, closing2, fill_holes, superimpose]\n",
    "        titles = ['0_original_image', '1_cropped_image', '2_binary_image', '3_remove_blobs', '4_closure', '5_roi', '6_erosion', '7_closure', '8_fill_hole', '9_result']\n",
    "        for i, title in enumerate(titles):\n",
    "            filename = directory1 + title + directory2\n",
    "            try:\n",
    "                cv.imwrite(filename, images[i])\n",
    "            except:\n",
    "                indices = images[i].astype(np.uint8)  #convert to an unsigned byte\n",
    "                indices*=255\n",
    "                cv.imwrite(filename, indices)\n",
    "    else:\n",
    "        #flip vertically\n",
    "        directory1 = 'preprocessing/pre1/'\n",
    "        directory2 = '.jpg'\n",
    "        images = [crop, binary, cleared, label_image, superimpose]\n",
    "        titles = ['cropped_image', 'binary_image', 'remove_blobs', 'label', 'result']\n",
    "\n",
    "        if save:\n",
    "            for y in range(5):\n",
    "                filename = directory1 + str(y+1) + titles[y] + '/' + titles[y] + str(num+1) + directory2\n",
    "                cv.imwrite(filename, images[y])\n",
    "\n",
    "    images = [im, crop, binary, cleared, closing, label_image, segmented_area, erosion, closing2, fill_holes, superimpose]\n",
    "    \n",
    "    if plot:\n",
    "        titles = ['Original Image', \n",
    "                 'Step 1: Cropped Image', \n",
    "                 'Step 2: Binary image', \n",
    "                 'Step 3: Remove blobs', \n",
    "                 'Step 4: Closure', \n",
    "                 'Step 5: Label', \n",
    "                 'Step 6: Region On Interest',\n",
    "                 'Step 7: Erosion',\n",
    "                 'Step 8: Closure', \n",
    "                 'Step 9: Fill Holes',\n",
    "                 'Step 10: Result']\n",
    "        plot_img(images, titles, camp=plt.cm.bone, rows = 3, cols = 4, fontsize= 50)\n",
    "    \n",
    "#     if show_on_window:\n",
    "#         directory1 = 'result/'\n",
    "#         directory2 = '.jpg'\n",
    "#         titles = ['0_original_image', '1_cropped_image', '2_binary_image', '3_remove_blobs', '4_closure', '5_roi', '6_erosion', '7_fill_hole', '8_result']\n",
    "#         for i, title in enumerate(titles):\n",
    "#             filename = directory1 + title + directory2\n",
    "#             try:\n",
    "#                 cv.imwrite(filename, images[i])\n",
    "#             except:\n",
    "#                 indices = images[i].astype(np.uint8)  #convert to an unsigned byte\n",
    "#                 indices*=255\n",
    "#                 cv.imwrite(filename, indices)\n",
    "    \n",
    "    return superimpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829c01f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply to all images\n",
    "X_segmented = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "\n",
    "for i, img in enumerate(X):\n",
    "    im = img.copy()\n",
    "    X_segmented[i] = get_segmented_lungs(im, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a22260",
   "metadata": {},
   "source": [
    "# Get Nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfdad3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to get the nodules\n",
    "def get_nodules_lungs(im, plot=False):\n",
    "    \n",
    "    #Step 1: Get Nodules in binary\n",
    "    binary = im < 140\n",
    "    \n",
    "    get_high_vals = binary == 0\n",
    "    \n",
    "    nodules = im.copy()\n",
    "    \n",
    "    nodules[nodules < 140] = 0\n",
    "    \n",
    "    if plot == True:\n",
    "        images = [im, binary, nodules]\n",
    "        titles = ['Segmented Image', 'Binary Nodules Image', 'Nodules Image']\n",
    "        plot_img(images, titles, rows = 1, cols = 3, fontsize= 10)\n",
    "        \n",
    "    return nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d93523e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nodules = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "\n",
    "for i, img in enumerate(X_segmented):\n",
    "    im = img.copy()\n",
    "    X_nodules[i] = get_nodules_lungs(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a87af8",
   "metadata": {},
   "source": [
    "# Get nodules further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "430ba8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1: Remove based on the region properties\n",
    "selem = ball(2)\n",
    "binary = binary_closing(X_nodules, selem)\n",
    "\n",
    "X_nodules_further = X_nodules.copy()\n",
    "\n",
    "label_scan = label(binary)\n",
    "\n",
    "areas = [r.area for r in regionprops(label_scan)]\n",
    "areas.sort()\n",
    "\n",
    "for r in regionprops(label_scan):\n",
    "    max_x, max_y, max_z = 0, 0, 0\n",
    "    min_x, min_y, min_z = 2000, 2000, 2000\n",
    "    \n",
    "    for c in r.coords:\n",
    "        max_z = max(c[0], max_z)\n",
    "        max_y = max(c[1], max_y)\n",
    "        max_x = max(c[2], max_x)\n",
    "        \n",
    "        min_z = min(c[0], min_z)\n",
    "        min_y = min(c[1], min_y)\n",
    "        min_x = min(c[2], min_x)\n",
    "    if (min_z == max_z or min_y == max_y or min_x == max_x or r.area > areas[-3]):\n",
    "        for c in r.coords:\n",
    "            X_nodules_further[c[0], c[1], c[2]] = 0\n",
    "    else:\n",
    "        index = (max((max_x - min_x), (max_y - min_y), (max_z - min_z))) / (min((max_x - min_x), (max_y - min_y) , (max_z - min_z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25ea32",
   "metadata": {},
   "source": [
    "# Get nodules opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3130b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: Using Opening method to remove\n",
    "selem = ball(2)\n",
    "X_nodules_opening = binary_closing(X_nodules, selem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f98b5b6",
   "metadata": {},
   "source": [
    "# Input image FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a99f4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variable to hold X & y\n",
    "#create numpy array placeholder for pixels with 1 channel(grayscale)\n",
    "IMG_SIZE = 512\n",
    "# CHANNEL = 1\n",
    "#arg: (length of numpy set, height, width, color channel)\n",
    "X_segmented_fft = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "X_segmented_fft_imaginer = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "X_segmented_fft_abs = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "\n",
    "X_nodules_fft = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "X_nodules_fft_imaginer = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "X_nodules_fft_abs = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "\n",
    "X_nodules_further_fft = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "X_nodules_further_fft_imaginer = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "X_nodules_further_fft_abs = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "\n",
    "X_nodules_opening_fft = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "X_nodules_opening_fft_imaginer = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "X_nodules_opening_fft_abs = np.empty((len(img_list), IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "\n",
    "index = range(42)\n",
    "\n",
    "for i, img_segmented,img_nodules, img_nodules_further,img_nodules_opening in zip(index, X_segmented, X_nodules, X_nodules_further, X_nodules_opening):\n",
    "    img_segmented = np.fft.fft2(img_segmented)\n",
    "    img_nodules = np.fft.fft2(img_nodules)\n",
    "    img_nodules_further = np.fft.fft2(img_nodules_further)\n",
    "    img_nodules_opening = np.fft.fft2(img_nodules_opening)\n",
    "    \n",
    "    X_segmented_fft[i] = img_segmented\n",
    "    X_nodules_fft[i] = img_nodules\n",
    "    X_nodules_further_fft[i] = img_nodules_further\n",
    "    X_nodules_opening_fft[i] = img_nodules_opening\n",
    "    \n",
    "    \n",
    "    X_segmented_fft_imaginer[i] = np.imag(img_segmented)\n",
    "    X_nodules_fft_imaginer[i] = np.imag(img_nodules)\n",
    "    X_nodules_further_fft_imaginer[i] = np.imag(img_nodules_further)\n",
    "    X_nodules_opening_fft_imaginer[i] = np.imag(img_nodules_opening)\n",
    "    \n",
    "    X_segmented_fft_abs[i] = np.abs(img_segmented)\n",
    "    X_nodules_fft_abs[i] = np.abs(img_nodules)\n",
    "    X_nodules_further_fft_abs[i] = np.abs(img_nodules_further)\n",
    "    X_nodules_opening_fft_abs[i] = np.abs(img_nodules_opening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9d87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "r_rscv = y.copy()\n",
    "y_rscv = pd.Series(r_rscv)\n",
    "y_rscv = r_rscv.factorize()\n",
    "\n",
    "# Convert y to one-hot encoding\n",
    "y_categorical = to_categorical(y_rscv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aaa629",
   "metadata": {},
   "source": [
    "To convert the `y` array to categorical values using `tf.keras.utils.to_categorical`, you can use the following code:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert y to one-hot encoding\n",
    "y_categorical = to_categorical(y)\n",
    "```\n",
    "\n",
    "This will convert the `y` array into a one-hot encoded representation where each row represents a sample and each column represents a class. The number of columns in the resulting array will be equal to the number of unique classes in the `y` array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2ade9",
   "metadata": {},
   "source": [
    "# Training Segmented FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e63ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 - 4s - loss: 1.0954 - accuracy: 0.4667 - val_loss: 1.5847 - val_accuracy: 0.3333 - 4s/epoch - 924ms/step\n",
      "Epoch 2/50\n",
      "4/4 - 1s - loss: 1.1704 - accuracy: 0.3333 - val_loss: 1.5953 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "Epoch 3/50\n",
      "4/4 - 1s - loss: 1.0948 - accuracy: 0.2333 - val_loss: 1.5790 - val_accuracy: 0.0000e+00 - 989ms/epoch - 247ms/step\n",
      "Epoch 4/50\n",
      "4/4 - 1s - loss: 1.0581 - accuracy: 0.4333 - val_loss: 1.6390 - val_accuracy: 0.0000e+00 - 1s/epoch - 272ms/step\n",
      "Epoch 5/50\n",
      "4/4 - 1s - loss: 1.0104 - accuracy: 0.5000 - val_loss: 1.6584 - val_accuracy: 0.0000e+00 - 1s/epoch - 255ms/step\n",
      "Epoch 6/50\n",
      "4/4 - 1s - loss: 1.0579 - accuracy: 0.4667 - val_loss: 1.7236 - val_accuracy: 0.0000e+00 - 1s/epoch - 251ms/step\n",
      "Epoch 7/50\n",
      "4/4 - 1s - loss: 1.0690 - accuracy: 0.4667 - val_loss: 1.7347 - val_accuracy: 0.0000e+00 - 1s/epoch - 251ms/step\n",
      "Epoch 8/50\n",
      "4/4 - 1s - loss: 1.0397 - accuracy: 0.4667 - val_loss: 1.7146 - val_accuracy: 0.0000e+00 - 989ms/epoch - 247ms/step\n",
      "Epoch 9/50\n",
      "4/4 - 1s - loss: 0.9725 - accuracy: 0.6000 - val_loss: 1.6655 - val_accuracy: 0.3333 - 965ms/epoch - 241ms/step\n",
      "Epoch 10/50\n",
      "4/4 - 1s - loss: 0.9531 - accuracy: 0.5667 - val_loss: 1.6151 - val_accuracy: 0.3333 - 973ms/epoch - 243ms/step\n",
      "Epoch 11/50\n",
      "4/4 - 1s - loss: 0.9245 - accuracy: 0.5333 - val_loss: 1.5836 - val_accuracy: 0.1667 - 941ms/epoch - 235ms/step\n",
      "Epoch 12/50\n",
      "4/4 - 1s - loss: 0.9707 - accuracy: 0.5667 - val_loss: 1.5686 - val_accuracy: 0.0000e+00 - 957ms/epoch - 239ms/step\n",
      "Epoch 13/50\n",
      "4/4 - 1s - loss: 1.0253 - accuracy: 0.4333 - val_loss: 1.6056 - val_accuracy: 0.0000e+00 - 972ms/epoch - 243ms/step\n",
      "Epoch 14/50\n",
      "4/4 - 1s - loss: 0.8872 - accuracy: 0.7333 - val_loss: 1.7038 - val_accuracy: 0.0000e+00 - 952ms/epoch - 238ms/step\n",
      "Epoch 15/50\n",
      "4/4 - 1s - loss: 0.9309 - accuracy: 0.6667 - val_loss: 1.4790 - val_accuracy: 0.0000e+00 - 925ms/epoch - 231ms/step\n",
      "Epoch 16/50\n",
      "4/4 - 1s - loss: 0.9550 - accuracy: 0.5333 - val_loss: 1.4605 - val_accuracy: 0.3333 - 974ms/epoch - 243ms/step\n",
      "Epoch 17/50\n",
      "4/4 - 1s - loss: 0.9055 - accuracy: 0.7333 - val_loss: 1.4868 - val_accuracy: 0.3333 - 973ms/epoch - 243ms/step\n",
      "Epoch 18/50\n",
      "4/4 - 1s - loss: 0.8884 - accuracy: 0.7000 - val_loss: 1.3765 - val_accuracy: 0.3333 - 957ms/epoch - 239ms/step\n",
      "Epoch 19/50\n",
      "4/4 - 1s - loss: 0.8724 - accuracy: 0.7000 - val_loss: 1.3916 - val_accuracy: 0.3333 - 941ms/epoch - 235ms/step\n",
      "Epoch 20/50\n",
      "4/4 - 1s - loss: 0.7927 - accuracy: 0.8333 - val_loss: 1.6890 - val_accuracy: 0.3333 - 966ms/epoch - 241ms/step\n",
      "Epoch 21/50\n",
      "4/4 - 1s - loss: 0.7743 - accuracy: 0.8000 - val_loss: 1.7782 - val_accuracy: 0.1667 - 957ms/epoch - 239ms/step\n",
      "Epoch 22/50\n",
      "4/4 - 1s - loss: 0.7436 - accuracy: 0.6333 - val_loss: 1.4228 - val_accuracy: 0.1667 - 958ms/epoch - 239ms/step\n",
      "Epoch 23/50\n",
      "4/4 - 1s - loss: 0.7253 - accuracy: 0.6000 - val_loss: 1.3267 - val_accuracy: 0.3333 - 957ms/epoch - 239ms/step\n",
      "Epoch 24/50\n",
      "4/4 - 1s - loss: 0.6652 - accuracy: 0.7667 - val_loss: 1.2549 - val_accuracy: 0.3333 - 958ms/epoch - 239ms/step\n",
      "Epoch 25/50\n",
      "4/4 - 1s - loss: 0.5907 - accuracy: 0.8333 - val_loss: 1.3513 - val_accuracy: 0.3333 - 951ms/epoch - 238ms/step\n",
      "Epoch 26/50\n",
      "4/4 - 1s - loss: 0.5532 - accuracy: 0.7667 - val_loss: 1.3465 - val_accuracy: 0.1667 - 958ms/epoch - 239ms/step\n",
      "Epoch 27/50\n",
      "4/4 - 1s - loss: 0.5126 - accuracy: 0.9333 - val_loss: 1.1295 - val_accuracy: 0.5000 - 941ms/epoch - 235ms/step\n",
      "Epoch 28/50\n",
      "4/4 - 1s - loss: 0.4064 - accuracy: 0.9000 - val_loss: 1.2092 - val_accuracy: 0.3333 - 957ms/epoch - 239ms/step\n",
      "Epoch 29/50\n",
      "4/4 - 1s - loss: 0.4002 - accuracy: 0.9333 - val_loss: 0.9300 - val_accuracy: 0.5000 - 957ms/epoch - 239ms/step\n",
      "Epoch 30/50\n",
      "4/4 - 1s - loss: 0.3371 - accuracy: 0.9333 - val_loss: 1.5191 - val_accuracy: 0.1667 - 953ms/epoch - 238ms/step\n",
      "Epoch 31/50\n",
      "4/4 - 1s - loss: 0.3365 - accuracy: 0.8667 - val_loss: 0.9711 - val_accuracy: 0.5000 - 942ms/epoch - 235ms/step\n",
      "Epoch 32/50\n",
      "4/4 - 1s - loss: 0.2867 - accuracy: 0.9000 - val_loss: 0.9119 - val_accuracy: 0.5000 - 942ms/epoch - 236ms/step\n",
      "Epoch 33/50\n",
      "4/4 - 1s - loss: 0.2062 - accuracy: 1.0000 - val_loss: 1.1171 - val_accuracy: 0.5000 - 989ms/epoch - 247ms/step\n",
      "Epoch 34/50\n",
      "4/4 - 1s - loss: 0.3675 - accuracy: 0.8333 - val_loss: 0.9779 - val_accuracy: 0.5000 - 941ms/epoch - 235ms/step\n",
      "Epoch 35/50\n",
      "4/4 - 1s - loss: 0.5602 - accuracy: 0.8000 - val_loss: 0.9273 - val_accuracy: 0.3333 - 964ms/epoch - 241ms/step\n",
      "Epoch 36/50\n",
      "4/4 - 1s - loss: 0.3298 - accuracy: 0.9667 - val_loss: 1.5553 - val_accuracy: 0.3333 - 957ms/epoch - 239ms/step\n",
      "Epoch 37/50\n",
      "4/4 - 1s - loss: 0.3353 - accuracy: 0.8667 - val_loss: 1.8147 - val_accuracy: 0.3333 - 972ms/epoch - 243ms/step\n",
      "Epoch 38/50\n",
      "4/4 - 1s - loss: 0.3761 - accuracy: 0.8667 - val_loss: 1.2783 - val_accuracy: 0.1667 - 959ms/epoch - 240ms/step\n",
      "Epoch 39/50\n",
      "4/4 - 1s - loss: 0.2354 - accuracy: 0.9000 - val_loss: 0.7215 - val_accuracy: 0.6667 - 956ms/epoch - 239ms/step\n",
      "Epoch 40/50\n",
      "4/4 - 1s - loss: 0.2023 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.8333 - 957ms/epoch - 239ms/step\n",
      "Epoch 41/50\n",
      "4/4 - 1s - loss: 0.1893 - accuracy: 0.9667 - val_loss: 0.7754 - val_accuracy: 0.5000 - 918ms/epoch - 229ms/step\n",
      "Epoch 42/50\n",
      "4/4 - 1s - loss: 0.1181 - accuracy: 1.0000 - val_loss: 0.5270 - val_accuracy: 0.8333 - 989ms/epoch - 247ms/step\n",
      "Epoch 43/50\n",
      "4/4 - 1s - loss: 0.1286 - accuracy: 1.0000 - val_loss: 0.8377 - val_accuracy: 0.5000 - 941ms/epoch - 235ms/step\n",
      "Epoch 44/50\n",
      "4/4 - 1s - loss: 0.1051 - accuracy: 1.0000 - val_loss: 0.5262 - val_accuracy: 0.8333 - 973ms/epoch - 243ms/step\n",
      "Epoch 45/50\n",
      "4/4 - 1s - loss: 0.1090 - accuracy: 0.9667 - val_loss: 0.5051 - val_accuracy: 1.0000 - 941ms/epoch - 235ms/step\n",
      "Epoch 46/50\n",
      "4/4 - 1s - loss: 0.1487 - accuracy: 0.9667 - val_loss: 0.5049 - val_accuracy: 0.8333 - 968ms/epoch - 242ms/step\n",
      "Epoch 47/50\n",
      "4/4 - 1s - loss: 0.1921 - accuracy: 0.9333 - val_loss: 0.9616 - val_accuracy: 0.5000 - 973ms/epoch - 243ms/step\n",
      "Epoch 48/50\n",
      "4/4 - 1s - loss: 0.1468 - accuracy: 0.9667 - val_loss: 1.0868 - val_accuracy: 0.5000 - 957ms/epoch - 239ms/step\n",
      "Epoch 49/50\n",
      "4/4 - 1s - loss: 0.1133 - accuracy: 1.0000 - val_loss: 0.4432 - val_accuracy: 0.8333 - 988ms/epoch - 247ms/step\n",
      "Epoch 50/50\n",
      "4/4 - 1s - loss: 0.1249 - accuracy: 1.0000 - val_loss: 0.4958 - val_accuracy: 0.6667 - 973ms/epoch - 243ms/step\n",
      "1/1 - 0s - loss: 1.0368 - accuracy: 0.3333 - 79ms/epoch - 79ms/step\n",
      "Test accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the data and labels\n",
    "data = X_segmented_fft\n",
    "labels = y_categorical\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_data = data[:30]\n",
    "train_labels = labels[:30]\n",
    "val_data = data[30:36]\n",
    "val_labels = labels[30:36]\n",
    "test_data = data[36:]\n",
    "test_labels = labels[36:]\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "train_data = train_data / 255.0\n",
    "val_data = val_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "# # Convert the labels to categorical format\n",
    "# train_labels = to_categorical(train_labels)\n",
    "# val_labels = to_categorical(val_labels)\n",
    "# test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(train_data.shape[1], train_data.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training set and validate on the validation set\n",
    "model.fit(train_data, train_labels, batch_size=8, epochs=50, validation_data=(val_data, val_labels), verbose=2)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef985bc5",
   "metadata": {},
   "source": [
    "# Training Nodules FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a134a71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 - 4s - loss: 1.0519 - accuracy: 0.4000 - val_loss: 1.6611 - val_accuracy: 0.0000e+00 - 4s/epoch - 915ms/step\n",
      "Epoch 2/50\n",
      "4/4 - 1s - loss: 1.0453 - accuracy: 0.4000 - val_loss: 1.6315 - val_accuracy: 0.0000e+00 - 957ms/epoch - 239ms/step\n",
      "Epoch 3/50\n",
      "4/4 - 1s - loss: 0.9720 - accuracy: 0.5000 - val_loss: 1.6120 - val_accuracy: 0.0000e+00 - 948ms/epoch - 237ms/step\n",
      "Epoch 4/50\n",
      "4/4 - 1s - loss: 0.9420 - accuracy: 0.6333 - val_loss: 1.5579 - val_accuracy: 0.3333 - 930ms/epoch - 233ms/step\n",
      "Epoch 5/50\n",
      "4/4 - 1s - loss: 1.0214 - accuracy: 0.4667 - val_loss: 1.5398 - val_accuracy: 0.3333 - 936ms/epoch - 234ms/step\n",
      "Epoch 6/50\n",
      "4/4 - 1s - loss: 0.9867 - accuracy: 0.4333 - val_loss: 1.6229 - val_accuracy: 0.0000e+00 - 941ms/epoch - 235ms/step\n",
      "Epoch 7/50\n",
      "4/4 - 1s - loss: 0.9226 - accuracy: 0.6000 - val_loss: 1.6618 - val_accuracy: 0.0000e+00 - 957ms/epoch - 239ms/step\n",
      "Epoch 8/50\n",
      "4/4 - 1s - loss: 0.9817 - accuracy: 0.5667 - val_loss: 1.6871 - val_accuracy: 0.0000e+00 - 967ms/epoch - 242ms/step\n",
      "Epoch 9/50\n",
      "4/4 - 1s - loss: 0.9370 - accuracy: 0.5667 - val_loss: 1.8349 - val_accuracy: 0.0000e+00 - 936ms/epoch - 234ms/step\n",
      "Epoch 10/50\n",
      "4/4 - 1s - loss: 0.9675 - accuracy: 0.4667 - val_loss: 1.6850 - val_accuracy: 0.0000e+00 - 973ms/epoch - 243ms/step\n",
      "Epoch 11/50\n",
      "4/4 - 1s - loss: 0.8768 - accuracy: 0.5333 - val_loss: 1.5096 - val_accuracy: 0.3333 - 941ms/epoch - 235ms/step\n",
      "Epoch 12/50\n",
      "4/4 - 1s - loss: 0.8704 - accuracy: 0.7000 - val_loss: 1.4452 - val_accuracy: 0.0000e+00 - 941ms/epoch - 235ms/step\n",
      "Epoch 13/50\n",
      "4/4 - 1s - loss: 0.8682 - accuracy: 0.6667 - val_loss: 1.5867 - val_accuracy: 0.0000e+00 - 950ms/epoch - 238ms/step\n",
      "Epoch 14/50\n",
      "4/4 - 1s - loss: 0.8763 - accuracy: 0.6000 - val_loss: 1.5786 - val_accuracy: 0.3333 - 957ms/epoch - 239ms/step\n",
      "Epoch 15/50\n",
      "4/4 - 1s - loss: 0.8460 - accuracy: 0.6333 - val_loss: 1.4466 - val_accuracy: 0.1667 - 989ms/epoch - 247ms/step\n",
      "Epoch 16/50\n",
      "4/4 - 1s - loss: 0.8631 - accuracy: 0.7333 - val_loss: 1.3943 - val_accuracy: 0.1667 - 973ms/epoch - 243ms/step\n",
      "Epoch 17/50\n",
      "4/4 - 1s - loss: 0.7766 - accuracy: 0.8000 - val_loss: 1.4828 - val_accuracy: 0.1667 - 988ms/epoch - 247ms/step\n",
      "Epoch 18/50\n",
      "4/4 - 1s - loss: 0.7015 - accuracy: 0.8667 - val_loss: 1.6009 - val_accuracy: 0.1667 - 1s/epoch - 258ms/step\n",
      "Epoch 19/50\n",
      "4/4 - 1s - loss: 0.7726 - accuracy: 0.7333 - val_loss: 1.2602 - val_accuracy: 0.3333 - 1s/epoch - 251ms/step\n",
      "Epoch 20/50\n",
      "4/4 - 1s - loss: 0.7786 - accuracy: 0.7333 - val_loss: 1.4197 - val_accuracy: 0.0000e+00 - 1s/epoch - 251ms/step\n",
      "Epoch 21/50\n",
      "4/4 - 1s - loss: 0.8584 - accuracy: 0.5000 - val_loss: 1.6021 - val_accuracy: 0.0000e+00 - 1s/epoch - 275ms/step\n",
      "Epoch 22/50\n",
      "4/4 - 1s - loss: 0.7556 - accuracy: 0.7667 - val_loss: 1.3153 - val_accuracy: 0.3333 - 1s/epoch - 251ms/step\n",
      "Epoch 23/50\n",
      "4/4 - 1s - loss: 0.8082 - accuracy: 0.6667 - val_loss: 1.5786 - val_accuracy: 0.3333 - 1s/epoch - 252ms/step\n",
      "Epoch 24/50\n",
      "4/4 - 1s - loss: 0.7061 - accuracy: 0.8000 - val_loss: 1.5128 - val_accuracy: 0.1667 - 1s/epoch - 263ms/step\n",
      "Epoch 25/50\n",
      "4/4 - 1s - loss: 0.6615 - accuracy: 0.8000 - val_loss: 1.1993 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "Epoch 26/50\n",
      "4/4 - 1s - loss: 0.6027 - accuracy: 0.8333 - val_loss: 1.5849 - val_accuracy: 0.1667 - 1s/epoch - 259ms/step\n",
      "Epoch 27/50\n",
      "4/4 - 1s - loss: 0.5644 - accuracy: 0.8000 - val_loss: 1.1503 - val_accuracy: 0.3333 - 1s/epoch - 259ms/step\n",
      "Epoch 28/50\n",
      "4/4 - 1s - loss: 0.5511 - accuracy: 0.8667 - val_loss: 1.2175 - val_accuracy: 0.3333 - 1s/epoch - 258ms/step\n",
      "Epoch 29/50\n",
      "4/4 - 1s - loss: 0.4491 - accuracy: 0.8667 - val_loss: 1.2773 - val_accuracy: 0.1667 - 1s/epoch - 279ms/step\n",
      "Epoch 30/50\n",
      "4/4 - 1s - loss: 0.4460 - accuracy: 0.9000 - val_loss: 0.9951 - val_accuracy: 0.6667 - 1s/epoch - 271ms/step\n",
      "Epoch 31/50\n",
      "4/4 - 1s - loss: 0.3986 - accuracy: 0.9667 - val_loss: 1.2552 - val_accuracy: 0.1667 - 1s/epoch - 270ms/step\n",
      "Epoch 32/50\n",
      "4/4 - 1s - loss: 0.4476 - accuracy: 0.8333 - val_loss: 1.1758 - val_accuracy: 0.3333 - 1s/epoch - 271ms/step\n",
      "Epoch 33/50\n",
      "4/4 - 1s - loss: 0.3022 - accuracy: 0.9333 - val_loss: 0.9002 - val_accuracy: 0.5000 - 1s/epoch - 276ms/step\n",
      "Epoch 34/50\n",
      "4/4 - 1s - loss: 0.3292 - accuracy: 0.9000 - val_loss: 0.9176 - val_accuracy: 0.6667 - 1s/epoch - 290ms/step\n",
      "Epoch 35/50\n",
      "4/4 - 1s - loss: 0.3001 - accuracy: 0.9333 - val_loss: 1.0512 - val_accuracy: 0.6667 - 1s/epoch - 279ms/step\n",
      "Epoch 36/50\n",
      "4/4 - 1s - loss: 0.2879 - accuracy: 0.9000 - val_loss: 0.9596 - val_accuracy: 0.6667 - 1s/epoch - 271ms/step\n",
      "Epoch 37/50\n",
      "4/4 - 1s - loss: 0.2163 - accuracy: 0.9667 - val_loss: 0.9614 - val_accuracy: 0.5000 - 1s/epoch - 270ms/step\n",
      "Epoch 38/50\n",
      "4/4 - 1s - loss: 0.2503 - accuracy: 0.9667 - val_loss: 0.9386 - val_accuracy: 0.6667 - 1s/epoch - 271ms/step\n",
      "Epoch 39/50\n",
      "4/4 - 1s - loss: 0.1768 - accuracy: 1.0000 - val_loss: 0.8506 - val_accuracy: 0.6667 - 1s/epoch - 282ms/step\n",
      "Epoch 40/50\n",
      "4/4 - 1s - loss: 0.1593 - accuracy: 0.9667 - val_loss: 0.6738 - val_accuracy: 0.6667 - 1s/epoch - 286ms/step\n",
      "Epoch 41/50\n",
      "4/4 - 1s - loss: 0.1461 - accuracy: 0.9667 - val_loss: 1.0730 - val_accuracy: 0.5000 - 1s/epoch - 291ms/step\n",
      "Epoch 42/50\n",
      "4/4 - 1s - loss: 0.1550 - accuracy: 1.0000 - val_loss: 0.8182 - val_accuracy: 0.6667 - 1s/epoch - 344ms/step\n",
      "Epoch 43/50\n",
      "4/4 - 1s - loss: 0.0894 - accuracy: 1.0000 - val_loss: 0.6507 - val_accuracy: 0.6667 - 1s/epoch - 306ms/step\n",
      "Epoch 44/50\n",
      "4/4 - 1s - loss: 0.0889 - accuracy: 1.0000 - val_loss: 0.9737 - val_accuracy: 0.5000 - 1s/epoch - 298ms/step\n",
      "Epoch 45/50\n",
      "4/4 - 1s - loss: 0.0754 - accuracy: 1.0000 - val_loss: 1.0131 - val_accuracy: 0.6667 - 1s/epoch - 283ms/step\n",
      "Epoch 46/50\n",
      "4/4 - 1s - loss: 0.0734 - accuracy: 1.0000 - val_loss: 0.6855 - val_accuracy: 0.8333 - 1s/epoch - 277ms/step\n",
      "Epoch 47/50\n",
      "4/4 - 1s - loss: 0.0767 - accuracy: 1.0000 - val_loss: 0.9166 - val_accuracy: 0.6667 - 1s/epoch - 279ms/step\n",
      "Epoch 48/50\n",
      "4/4 - 1s - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.6669 - val_accuracy: 0.6667 - 1s/epoch - 291ms/step\n",
      "Epoch 49/50\n",
      "4/4 - 1s - loss: 0.0487 - accuracy: 1.0000 - val_loss: 0.8689 - val_accuracy: 0.6667 - 1s/epoch - 291ms/step\n",
      "Epoch 50/50\n",
      "4/4 - 1s - loss: 0.0461 - accuracy: 1.0000 - val_loss: 1.1394 - val_accuracy: 0.6667 - 1s/epoch - 288ms/step\n",
      "1/1 - 0s - loss: 2.2081 - accuracy: 0.3333 - 93ms/epoch - 93ms/step\n",
      "Test accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the data and labels\n",
    "data = X_nodules_fft\n",
    "labels = y_categorical\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_data = data[:30]\n",
    "train_labels = labels[:30]\n",
    "val_data = data[30:36]\n",
    "val_labels = labels[30:36]\n",
    "test_data = data[36:]\n",
    "test_labels = labels[36:]\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "train_data = train_data / 255.0\n",
    "val_data = val_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(train_data.shape[1], train_data.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training set and validate on the validation set\n",
    "model.fit(train_data, train_labels, batch_size=8, epochs=50, validation_data=(val_data, val_labels), verbose=2)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f1a42",
   "metadata": {},
   "source": [
    "# Training Nodules Further FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cf12d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 - 4s - loss: 1.1174 - accuracy: 0.3333 - val_loss: 1.6356 - val_accuracy: 0.0000e+00 - 4s/epoch - 1s/step\n",
      "Epoch 2/50\n",
      "4/4 - 1s - loss: 0.9896 - accuracy: 0.5000 - val_loss: 1.6080 - val_accuracy: 0.3333 - 1s/epoch - 263ms/step\n",
      "Epoch 3/50\n",
      "4/4 - 1s - loss: 0.9874 - accuracy: 0.4000 - val_loss: 1.6456 - val_accuracy: 0.3333 - 1s/epoch - 274ms/step\n",
      "Epoch 4/50\n",
      "4/4 - 1s - loss: 1.0027 - accuracy: 0.4667 - val_loss: 1.6039 - val_accuracy: 0.0000e+00 - 1s/epoch - 279ms/step\n",
      "Epoch 5/50\n",
      "4/4 - 1s - loss: 1.0201 - accuracy: 0.4333 - val_loss: 1.5591 - val_accuracy: 0.0000e+00 - 1s/epoch - 263ms/step\n",
      "Epoch 6/50\n",
      "4/4 - 1s - loss: 0.9422 - accuracy: 0.5333 - val_loss: 1.5253 - val_accuracy: 0.0000e+00 - 1s/epoch - 261ms/step\n",
      "Epoch 7/50\n",
      "4/4 - 1s - loss: 1.0541 - accuracy: 0.3667 - val_loss: 1.5375 - val_accuracy: 0.3333 - 1s/epoch - 263ms/step\n",
      "Epoch 8/50\n",
      "4/4 - 1s - loss: 0.9674 - accuracy: 0.5667 - val_loss: 1.5768 - val_accuracy: 0.0000e+00 - 1s/epoch - 267ms/step\n",
      "Epoch 9/50\n",
      "4/4 - 1s - loss: 1.0050 - accuracy: 0.4333 - val_loss: 1.6232 - val_accuracy: 0.0000e+00 - 1s/epoch - 271ms/step\n",
      "Epoch 10/50\n",
      "4/4 - 1s - loss: 0.9364 - accuracy: 0.6000 - val_loss: 1.6142 - val_accuracy: 0.1667 - 1s/epoch - 267ms/step\n",
      "Epoch 11/50\n",
      "4/4 - 1s - loss: 0.9465 - accuracy: 0.5667 - val_loss: 1.6411 - val_accuracy: 0.3333 - 1s/epoch - 269ms/step\n",
      "Epoch 12/50\n",
      "4/4 - 1s - loss: 0.8944 - accuracy: 0.6000 - val_loss: 1.6430 - val_accuracy: 0.3333 - 1s/epoch - 267ms/step\n",
      "Epoch 13/50\n",
      "4/4 - 1s - loss: 0.8619 - accuracy: 0.7667 - val_loss: 1.6758 - val_accuracy: 0.1667 - 1s/epoch - 275ms/step\n",
      "Epoch 14/50\n",
      "4/4 - 1s - loss: 0.8782 - accuracy: 0.5667 - val_loss: 1.6886 - val_accuracy: 0.3333 - 1s/epoch - 271ms/step\n",
      "Epoch 15/50\n",
      "4/4 - 1s - loss: 0.8616 - accuracy: 0.6333 - val_loss: 1.7247 - val_accuracy: 0.1667 - 1s/epoch - 265ms/step\n",
      "Epoch 16/50\n",
      "4/4 - 1s - loss: 0.8278 - accuracy: 0.7333 - val_loss: 1.7035 - val_accuracy: 0.0000e+00 - 1s/epoch - 271ms/step\n",
      "Epoch 17/50\n",
      "4/4 - 1s - loss: 0.8029 - accuracy: 0.7000 - val_loss: 1.6264 - val_accuracy: 0.0000e+00 - 1s/epoch - 267ms/step\n",
      "Epoch 18/50\n",
      "4/4 - 1s - loss: 0.7813 - accuracy: 0.6333 - val_loss: 1.6186 - val_accuracy: 0.3333 - 1s/epoch - 263ms/step\n",
      "Epoch 19/50\n",
      "4/4 - 1s - loss: 0.7199 - accuracy: 0.8000 - val_loss: 1.5778 - val_accuracy: 0.3333 - 1s/epoch - 278ms/step\n",
      "Epoch 20/50\n",
      "4/4 - 1s - loss: 0.6849 - accuracy: 0.8333 - val_loss: 1.6935 - val_accuracy: 0.0000e+00 - 1s/epoch - 270ms/step\n",
      "Epoch 21/50\n",
      "4/4 - 1s - loss: 0.7236 - accuracy: 0.7333 - val_loss: 1.7192 - val_accuracy: 0.0000e+00 - 1s/epoch - 267ms/step\n",
      "Epoch 22/50\n",
      "4/4 - 1s - loss: 0.7325 - accuracy: 0.7000 - val_loss: 1.5970 - val_accuracy: 0.3333 - 1s/epoch - 271ms/step\n",
      "Epoch 23/50\n",
      "4/4 - 1s - loss: 0.5735 - accuracy: 0.8333 - val_loss: 1.4468 - val_accuracy: 0.0000e+00 - 1s/epoch - 263ms/step\n",
      "Epoch 24/50\n",
      "4/4 - 1s - loss: 0.6328 - accuracy: 0.8333 - val_loss: 1.6003 - val_accuracy: 0.3333 - 1s/epoch - 263ms/step\n",
      "Epoch 25/50\n",
      "4/4 - 1s - loss: 0.4691 - accuracy: 0.8667 - val_loss: 1.5660 - val_accuracy: 0.3333 - 1s/epoch - 261ms/step\n",
      "Epoch 26/50\n",
      "4/4 - 1s - loss: 0.4429 - accuracy: 0.8667 - val_loss: 1.6503 - val_accuracy: 0.0000e+00 - 1s/epoch - 263ms/step\n",
      "Epoch 27/50\n",
      "4/4 - 1s - loss: 0.4061 - accuracy: 0.8667 - val_loss: 1.7155 - val_accuracy: 0.0000e+00 - 1s/epoch - 257ms/step\n",
      "Epoch 28/50\n",
      "4/4 - 1s - loss: 0.4142 - accuracy: 0.8667 - val_loss: 1.7458 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "Epoch 29/50\n",
      "4/4 - 1s - loss: 0.5565 - accuracy: 0.8000 - val_loss: 1.2393 - val_accuracy: 0.3333 - 1s/epoch - 258ms/step\n",
      "Epoch 30/50\n",
      "4/4 - 1s - loss: 0.4475 - accuracy: 0.9000 - val_loss: 1.3914 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "Epoch 31/50\n",
      "4/4 - 1s - loss: 0.5056 - accuracy: 0.8000 - val_loss: 1.4340 - val_accuracy: 0.3333 - 1s/epoch - 265ms/step\n",
      "Epoch 32/50\n",
      "4/4 - 1s - loss: 0.4741 - accuracy: 0.8000 - val_loss: 1.2773 - val_accuracy: 0.3333 - 1s/epoch - 263ms/step\n",
      "Epoch 33/50\n",
      "4/4 - 1s - loss: 0.3533 - accuracy: 0.9333 - val_loss: 1.8827 - val_accuracy: 0.0000e+00 - 1s/epoch - 263ms/step\n",
      "Epoch 34/50\n",
      "4/4 - 1s - loss: 0.4040 - accuracy: 0.8667 - val_loss: 1.4913 - val_accuracy: 0.3333 - 1s/epoch - 261ms/step\n",
      "Epoch 35/50\n",
      "4/4 - 1s - loss: 0.3498 - accuracy: 0.8667 - val_loss: 1.2293 - val_accuracy: 0.3333 - 1s/epoch - 259ms/step\n",
      "Epoch 36/50\n",
      "4/4 - 1s - loss: 0.2860 - accuracy: 0.9333 - val_loss: 1.5262 - val_accuracy: 0.1667 - 1s/epoch - 259ms/step\n",
      "Epoch 37/50\n",
      "4/4 - 1s - loss: 0.2733 - accuracy: 0.8667 - val_loss: 1.2640 - val_accuracy: 0.1667 - 1s/epoch - 251ms/step\n",
      "Epoch 38/50\n",
      "4/4 - 1s - loss: 0.2277 - accuracy: 0.9333 - val_loss: 1.1904 - val_accuracy: 0.1667 - 1s/epoch - 250ms/step\n",
      "Epoch 39/50\n",
      "4/4 - 1s - loss: 0.1908 - accuracy: 0.9667 - val_loss: 1.3862 - val_accuracy: 0.1667 - 1s/epoch - 251ms/step\n",
      "Epoch 40/50\n",
      "4/4 - 1s - loss: 0.2216 - accuracy: 0.9000 - val_loss: 1.0296 - val_accuracy: 0.3333 - 1s/epoch - 251ms/step\n",
      "Epoch 41/50\n",
      "4/4 - 1s - loss: 0.2052 - accuracy: 1.0000 - val_loss: 1.4569 - val_accuracy: 0.1667 - 1s/epoch - 255ms/step\n",
      "Epoch 42/50\n",
      "4/4 - 1s - loss: 0.2077 - accuracy: 0.9000 - val_loss: 1.8175 - val_accuracy: 0.1667 - 1s/epoch - 255ms/step\n",
      "Epoch 43/50\n",
      "4/4 - 1s - loss: 0.1642 - accuracy: 0.9667 - val_loss: 0.9916 - val_accuracy: 0.5000 - 1s/epoch - 255ms/step\n",
      "Epoch 44/50\n",
      "4/4 - 1s - loss: 0.1699 - accuracy: 0.9667 - val_loss: 1.4782 - val_accuracy: 0.1667 - 1s/epoch - 253ms/step\n",
      "Epoch 45/50\n",
      "4/4 - 1s - loss: 0.2172 - accuracy: 1.0000 - val_loss: 1.7691 - val_accuracy: 0.0000e+00 - 1s/epoch - 251ms/step\n",
      "Epoch 46/50\n",
      "4/4 - 1s - loss: 0.1774 - accuracy: 0.9333 - val_loss: 1.5191 - val_accuracy: 0.1667 - 989ms/epoch - 247ms/step\n",
      "Epoch 47/50\n",
      "4/4 - 1s - loss: 0.1095 - accuracy: 1.0000 - val_loss: 1.5199 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "Epoch 48/50\n",
      "4/4 - 1s - loss: 0.0871 - accuracy: 1.0000 - val_loss: 1.2219 - val_accuracy: 0.1667 - 989ms/epoch - 247ms/step\n",
      "Epoch 49/50\n",
      "4/4 - 1s - loss: 0.0867 - accuracy: 1.0000 - val_loss: 1.6958 - val_accuracy: 0.1667 - 1000ms/epoch - 250ms/step\n",
      "Epoch 50/50\n",
      "4/4 - 1s - loss: 0.0676 - accuracy: 1.0000 - val_loss: 1.4420 - val_accuracy: 0.1667 - 1s/epoch - 253ms/step\n",
      "1/1 - 0s - loss: 2.8893 - accuracy: 0.0000e+00 - 79ms/epoch - 79ms/step\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the data and labels\n",
    "data = X_nodules_further_fft\n",
    "labels = y_categorical\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_data = data[:30]\n",
    "train_labels = labels[:30]\n",
    "val_data = data[30:36]\n",
    "val_labels = labels[30:36]\n",
    "test_data = data[36:]\n",
    "test_labels = labels[36:]\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "train_data = train_data / 255.0\n",
    "val_data = val_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "# # Convert the labels to categorical format\n",
    "# train_labels = to_categorical(train_labels)\n",
    "# val_labels = to_categorical(val_labels)\n",
    "# test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(train_data.shape[1], train_data.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training set and validate on the validation set\n",
    "model.fit(train_data, train_labels, batch_size=8, epochs=50, validation_data=(val_data, val_labels), verbose=2)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446bdec",
   "metadata": {},
   "source": [
    "# Training Nodules Opening FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2779ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 - 4s - loss: 1.1286 - accuracy: 0.2000 - val_loss: 1.7969 - val_accuracy: 0.0000e+00 - 4s/epoch - 948ms/step\n",
      "Epoch 2/50\n",
      "4/4 - 1s - loss: 0.9789 - accuracy: 0.4667 - val_loss: 1.7450 - val_accuracy: 0.0000e+00 - 1s/epoch - 255ms/step\n",
      "Epoch 3/50\n",
      "4/4 - 1s - loss: 1.0193 - accuracy: 0.5333 - val_loss: 1.7179 - val_accuracy: 0.0000e+00 - 1s/epoch - 255ms/step\n",
      "Epoch 4/50\n",
      "4/4 - 1s - loss: 0.9311 - accuracy: 0.6667 - val_loss: 1.6316 - val_accuracy: 0.0000e+00 - 1s/epoch - 255ms/step\n",
      "Epoch 5/50\n",
      "4/4 - 1s - loss: 0.9761 - accuracy: 0.6000 - val_loss: 1.6151 - val_accuracy: 0.0000e+00 - 1s/epoch - 270ms/step\n",
      "Epoch 6/50\n",
      "4/4 - 1s - loss: 0.9558 - accuracy: 0.5333 - val_loss: 1.5342 - val_accuracy: 0.0000e+00 - 1s/epoch - 257ms/step\n",
      "Epoch 7/50\n",
      "4/4 - 1s - loss: 0.9560 - accuracy: 0.5333 - val_loss: 1.6363 - val_accuracy: 0.0000e+00 - 1s/epoch - 263ms/step\n",
      "Epoch 8/50\n",
      "4/4 - 1s - loss: 0.9633 - accuracy: 0.5333 - val_loss: 1.6239 - val_accuracy: 0.0000e+00 - 1s/epoch - 263ms/step\n",
      "Epoch 9/50\n",
      "4/4 - 1s - loss: 0.9165 - accuracy: 0.6333 - val_loss: 1.5712 - val_accuracy: 0.0000e+00 - 1s/epoch - 267ms/step\n",
      "Epoch 10/50\n",
      "4/4 - 1s - loss: 0.8880 - accuracy: 0.6000 - val_loss: 1.5971 - val_accuracy: 0.3333 - 1s/epoch - 261ms/step\n",
      "Epoch 11/50\n",
      "4/4 - 1s - loss: 0.8819 - accuracy: 0.7333 - val_loss: 1.3751 - val_accuracy: 0.1667 - 1s/epoch - 251ms/step\n",
      "Epoch 12/50\n",
      "4/4 - 1s - loss: 0.8407 - accuracy: 0.7333 - val_loss: 1.7181 - val_accuracy: 0.0000e+00 - 1s/epoch - 251ms/step\n",
      "Epoch 13/50\n",
      "4/4 - 1s - loss: 0.7510 - accuracy: 0.8667 - val_loss: 1.4039 - val_accuracy: 0.1667 - 1s/epoch - 259ms/step\n",
      "Epoch 14/50\n",
      "4/4 - 1s - loss: 0.6989 - accuracy: 0.7667 - val_loss: 1.4950 - val_accuracy: 0.1667 - 1s/epoch - 255ms/step\n",
      "Epoch 15/50\n",
      "4/4 - 1s - loss: 0.6459 - accuracy: 0.7667 - val_loss: 1.5370 - val_accuracy: 0.1667 - 1s/epoch - 253ms/step\n",
      "Epoch 16/50\n",
      "4/4 - 1s - loss: 0.6200 - accuracy: 0.7333 - val_loss: 1.3984 - val_accuracy: 0.3333 - 1s/epoch - 251ms/step\n",
      "Epoch 17/50\n",
      "4/4 - 1s - loss: 0.7265 - accuracy: 0.6000 - val_loss: 1.4593 - val_accuracy: 0.0000e+00 - 1s/epoch - 259ms/step\n",
      "Epoch 18/50\n",
      "4/4 - 1s - loss: 0.5868 - accuracy: 0.7667 - val_loss: 1.7547 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "Epoch 19/50\n",
      "4/4 - 1s - loss: 0.5437 - accuracy: 0.8667 - val_loss: 1.6151 - val_accuracy: 0.0000e+00 - 1s/epoch - 255ms/step\n",
      "Epoch 20/50\n",
      "4/4 - 1s - loss: 0.4822 - accuracy: 0.8667 - val_loss: 1.5579 - val_accuracy: 0.3333 - 1s/epoch - 258ms/step\n",
      "Epoch 21/50\n",
      "4/4 - 1s - loss: 0.3626 - accuracy: 0.8667 - val_loss: 1.3636 - val_accuracy: 0.0000e+00 - 1s/epoch - 259ms/step\n",
      "Epoch 22/50\n",
      "4/4 - 1s - loss: 0.3909 - accuracy: 0.9667 - val_loss: 1.7355 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "Epoch 23/50\n",
      "4/4 - 1s - loss: 0.2536 - accuracy: 0.9667 - val_loss: 1.5124 - val_accuracy: 0.0000e+00 - 1s/epoch - 255ms/step\n",
      "Epoch 24/50\n",
      "4/4 - 1s - loss: 0.2855 - accuracy: 0.9667 - val_loss: 1.3141 - val_accuracy: 0.1667 - 1s/epoch - 251ms/step\n",
      "Epoch 25/50\n",
      "4/4 - 1s - loss: 0.2185 - accuracy: 0.9333 - val_loss: 1.0857 - val_accuracy: 0.5000 - 998ms/epoch - 249ms/step\n",
      "Epoch 26/50\n",
      "4/4 - 1s - loss: 0.2368 - accuracy: 0.9667 - val_loss: 1.5998 - val_accuracy: 0.1667 - 1s/epoch - 255ms/step\n",
      "Epoch 27/50\n",
      "4/4 - 1s - loss: 0.2775 - accuracy: 0.9667 - val_loss: 1.6102 - val_accuracy: 0.1667 - 1s/epoch - 255ms/step\n",
      "Epoch 28/50\n",
      "4/4 - 1s - loss: 0.1481 - accuracy: 0.9667 - val_loss: 1.4456 - val_accuracy: 0.1667 - 1s/epoch - 255ms/step\n",
      "Epoch 29/50\n",
      "4/4 - 1s - loss: 0.1427 - accuracy: 1.0000 - val_loss: 1.4059 - val_accuracy: 0.1667 - 1s/epoch - 255ms/step\n",
      "Epoch 30/50\n",
      "4/4 - 1s - loss: 0.0892 - accuracy: 1.0000 - val_loss: 1.5680 - val_accuracy: 0.1667 - 1s/epoch - 258ms/step\n",
      "Epoch 31/50\n",
      "4/4 - 1s - loss: 0.0993 - accuracy: 0.9667 - val_loss: 1.1606 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "Epoch 32/50\n",
      "4/4 - 1s - loss: 0.0897 - accuracy: 1.0000 - val_loss: 1.2813 - val_accuracy: 0.3333 - 988ms/epoch - 247ms/step\n",
      "Epoch 33/50\n",
      "4/4 - 1s - loss: 0.0500 - accuracy: 1.0000 - val_loss: 2.5569 - val_accuracy: 0.1667 - 1s/epoch - 255ms/step\n",
      "Epoch 34/50\n",
      "4/4 - 1s - loss: 0.0442 - accuracy: 1.0000 - val_loss: 1.4617 - val_accuracy: 0.3333 - 1s/epoch - 251ms/step\n",
      "Epoch 35/50\n",
      "4/4 - 1s - loss: 0.0504 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 0.6667 - 1s/epoch - 254ms/step\n",
      "Epoch 36/50\n",
      "4/4 - 1s - loss: 0.0384 - accuracy: 1.0000 - val_loss: 2.2409 - val_accuracy: 0.3333 - 1s/epoch - 259ms/step\n",
      "Epoch 37/50\n",
      "4/4 - 1s - loss: 0.0227 - accuracy: 1.0000 - val_loss: 1.5667 - val_accuracy: 0.3333 - 1s/epoch - 263ms/step\n",
      "Epoch 38/50\n",
      "4/4 - 1s - loss: 0.0166 - accuracy: 1.0000 - val_loss: 1.1393 - val_accuracy: 0.5000 - 1s/epoch - 259ms/step\n",
      "Epoch 39/50\n",
      "4/4 - 1s - loss: 0.0252 - accuracy: 1.0000 - val_loss: 1.1806 - val_accuracy: 0.5000 - 1s/epoch - 259ms/step\n",
      "Epoch 40/50\n",
      "4/4 - 1s - loss: 0.0185 - accuracy: 1.0000 - val_loss: 2.0821 - val_accuracy: 0.3333 - 1s/epoch - 257ms/step\n",
      "Epoch 41/50\n",
      "4/4 - 1s - loss: 0.0149 - accuracy: 1.0000 - val_loss: 2.5196 - val_accuracy: 0.1667 - 1s/epoch - 251ms/step\n",
      "Epoch 42/50\n",
      "4/4 - 1s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 2.4026 - val_accuracy: 0.1667 - 1s/epoch - 251ms/step\n",
      "Epoch 43/50\n",
      "4/4 - 1s - loss: 0.0141 - accuracy: 1.0000 - val_loss: 2.5403 - val_accuracy: 0.1667 - 1s/epoch - 251ms/step\n",
      "Epoch 44/50\n",
      "4/4 - 1s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 2.3147 - val_accuracy: 0.1667 - 1s/epoch - 257ms/step\n",
      "Epoch 45/50\n",
      "4/4 - 1s - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.8760 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "Epoch 46/50\n",
      "4/4 - 1s - loss: 0.0118 - accuracy: 1.0000 - val_loss: 1.5267 - val_accuracy: 0.3333 - 1s/epoch - 262ms/step\n",
      "Epoch 47/50\n",
      "4/4 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.5176 - val_accuracy: 0.3333 - 1s/epoch - 259ms/step\n",
      "Epoch 48/50\n",
      "4/4 - 1s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.8252 - val_accuracy: 0.3333 - 1s/epoch - 263ms/step\n",
      "Epoch 49/50\n",
      "4/4 - 1s - loss: 0.0152 - accuracy: 1.0000 - val_loss: 2.0120 - val_accuracy: 0.3333 - 1s/epoch - 274ms/step\n",
      "Epoch 50/50\n",
      "4/4 - 1s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.7000 - val_accuracy: 0.3333 - 1s/epoch - 255ms/step\n",
      "1/1 - 0s - loss: 3.3288 - accuracy: 0.0000e+00 - 110ms/epoch - 110ms/step\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the data and labels\n",
    "data = X_nodules_opening_fft\n",
    "labels = y_categorical\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_data = data[:30]\n",
    "train_labels = labels[:30]\n",
    "val_data = data[30:36]\n",
    "val_labels = labels[30:36]\n",
    "test_data = data[36:]\n",
    "test_labels = labels[36:]\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "train_data = train_data / 255.0\n",
    "val_data = val_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "# # Convert the labels to categorical format\n",
    "# train_labels = to_categorical(train_labels)\n",
    "# val_labels = to_categorical(val_labels)\n",
    "# test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(train_data.shape[1], train_data.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training set and validate on the validation set\n",
    "model.fit(train_data, train_labels, batch_size=8, epochs=50, validation_data=(val_data, val_labels), verbose=2)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497f86a",
   "metadata": {},
   "source": [
    "# Perform a gridsearch technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d173dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the data and labels\n",
    "data = X_nodules_fft\n",
    "labels = y_categorical\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_data = data[:30]\n",
    "train_labels = labels[:30]\n",
    "val_data = data[30:36]\n",
    "val_labels = labels[30:36]\n",
    "test_data = data[36:]\n",
    "test_labels = labels[36:]\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "train_data = train_data / 255.0\n",
    "val_data = val_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "# Define the model\n",
    "def create_model(neurons=64, lstm_layers=1, ann_layers=1, activation='softmax', dropout_layers=0, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    for i in range(lstm_layers):\n",
    "        model.add(LSTM(neurons, input_shape=(train_data.shape[1], train_data.shape[2]), return_sequences=True if i < lstm_layers-1 else False))\n",
    "        model.add(Dropout(0.2))\n",
    "    for i in range(ann_layers):\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "        model.add(Dropout(0.2))\n",
    "    for i in range(dropout_layers):\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'neurons': [32, 64, 128],\n",
    "    'lstm_layers': [1, 2, 3],\n",
    "    'ann_layers': [1, 2, 3],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh'],\n",
    "    'dropout_layers': [0, 1, 2],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'epochs': [50, 100],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# Define the grid search object\n",
    "grid = GridSearchCV(estimator=KerasClassifier(build_fn=create_model), param_grid=param_grid, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search object on the training set and validate on the validation set\n",
    "grid_result = grid.fit(train_data, train_labels, validation_data=(val_data, val_labels))\n",
    "\n",
    "# Print the best result\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss, test_acc = grid_result.best_estimator_.model.evaluate(test_data, test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
